<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Modern Causal Inference</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="32b4128f-336d-4e47-94fc-e56d2de18ec5" class="page sans"><header><img class="page-cover-image" src="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/Untitled-2021-10-05-0651.png" style="object-position:center 11.429999999999996%"/><h1 class="page-title">Modern Causal Inference</h1></header><div class="page-body"><blockquote id="b282f503-8e00-4a9d-a968-333b7e9f9cf9" class=""><strong>Alejandro Schuler</strong></blockquote><p id="af8dcc60-f1a6-4816-ab9f-dc81d434024c" class="">
</p><ul id="057e869d-2f08-4954-a789-d0b15da4fbaf" class="block-color-gray_background toggle"><li><details open=""><summary><mark class="highlight-gray"><strong>Table of Contents</strong></mark></summary><nav id="ae1cac45-8754-4227-9abe-fc0909d7b4c4" class="block-color-gray_background table_of_contents"><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#9591aa3b-830c-43b5-ad32-d1b389159df4">Why this book?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#43fc62f6-65c7-4896-ab1f-0f2cba6a9dc1">A rational paradigm for statistics</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#706acc76-a274-4eb1-8e25-ccdaa6d61d64">Accessibility to newcomers</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#5477b83f-2954-4e42-a823-fd5beda84472">What&#x27;s in this book?</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#794d3642-6573-41be-9b45-2053a0822643">Who is it for?</a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#bf04ef60-09fd-44f8-b7b5-2402a8c047fb">Hard Prerequisites </a></div><div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href="#c48190eb-8082-4c12-bf3c-c363fbcdb3c9">Soft Prerequisites</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#5c0297e8-c61f-4149-b71f-21ad21c4411f">What&#x27;s <em>not</em> in this book?</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#f08da3af-a4a5-428e-b48e-6b7de4335373">Credit Due</a></div><div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href="#530c6206-2f7f-428e-864b-f6b4d49dc0e0">Alternatives</a></div></nav></details></li></ul><h1 id="9591aa3b-830c-43b5-ad32-d1b389159df4" class="">Why this book?</h1><p id="330a3229-0aa7-4f19-99db-12319f31af8e" class="">My goal is to get people with any amount of experience in formal mathematics past undergraduate probability to be able to take a vaguely-defined scientific question, translate it into a formal statistical problem, and solve it using a tool that is optimally constructed for the job at hand. I&#x27;ll try to give you an idea of why I think that’s a worthy goal and why I think other resources fall a little short.</p><h2 id="43fc62f6-65c7-4896-ab1f-0f2cba6a9dc1" class="">A rational paradigm for statistics</h2><p id="ffc3c6cf-bf8d-4521-938b-c5a9a1e28e61" class="">I&#x27;ll speak in great generality here (because there are many resources that I do love dearly), but the majority of texts, curricula, and articles out there don’t  often enough emphasize three important points that I think are at the center of good statistics. </p><p id="3f8953aa-b91e-48c7-abc7-314cfd05bd39" class="">The first is that for all practical purposes, <strong>the point of statistics </strong><strong><em>is</em></strong><strong> causal inference</strong>. Ultimately, we humans are concerned with how to make decisions under uncertainty that lead to the best outcome. These are fundamentally causal questions that ask &quot;<em>what if</em>&quot; we did A instead of B? The machinery of statistical inference is agnostic to causality, but that doesn&#x27;t change the fact that our motivation in using it isn&#x27;t. Therefore there is little point in shying away from causal claims because noncausal claims are not often of any practical utility. </p><p id="f92d06d5-20b2-4ce8-876e-99233f408b3a" class="">That leads to the second, often misunderstood point, which is that <strong>there is no such thing as a method for causal inference</strong>. This isn&#x27;t a failing of the statistics literature so much as a failing of the naive popularization of new ideas (e.g. blog posts, low-quality papers). As I said, the process of statistical inference (for point estimation) only cares about estimating a parameter of a probability distribution and quantifying the sampling distribution of that estimate. There is nothing &quot;causal&quot; about that, and many are surprised to learn that the algorithms used for &quot;causal&quot; inference are actually identical to those used to make noncausal statements. What makes an analysis causal has little if anything to do with the estimator or algorithm used. It has everything to do with whether or not the researcher can satisfy certain non-testable assumptions about the process that generated the observed data. It&#x27;s important to keep these things (statistical inference and causal identification) separated in your mind even though they must always work together.</p><p id="c6071344-1525-4f67-84f1-fd919c4cbbb9" class="">The last and perhaps greatest shortcoming of traditional statistics pedagogy is that it generally does not teach you to ask the question that makes sense scientifically and <em>then</em> translate it into a statistical formalism. This is partly because, in the past, our analytic tools were quite limited and no progress could be made unless one imposed unrealistic assumptions or changed the question to fit the existing methods. Today, however, <strong>we have powerful tools that can then translate a given statistical problem into an optimal method for estimation </strong>in a wide variety of settings. It&#x27;s not always totally automatic, but at a minimum it provides a clear way to think about what&#x27;s better and what&#x27;s worse. It liberates you from the route if-this-kind-of-data-then-this-method thinking that only makes for bad statistics and bad science.</p><p id="273ffc3c-429d-4399-be11-caaead2ab27a" class="">Taken together, these three points give this book a clear, unified, and increasingly popular perspective on causal inference. I have no doubt whatsoever that this is the paradigm that will come to dominate common practice in the next decades and century. While many other books and articles implicitly take this view, here I emphasize it explicitly to make it easier for you to understand what it is and what&#x27;s different about it relative to the way things used to be done.</p><h2 id="706acc76-a274-4eb1-8e25-ccdaa6d61d64" class="">Accessibility to newcomers</h2><p id="5d8f3118-1100-4fef-8e41-8634e124fbfa" class="">Besides not being explicit enough, <strong>a</strong> <strong>shortcoming of the literature on modern causal inference is that it is dauntingly inaccessible to anyone without years of formal mathematical training at the graduate level</strong>. The existing resources tend to be either so high-level that the reader does not feel that they have actually understood anything or so crammed with proofs, notation, and unfamiliar mathematical abstractions that the reader is immediately turned off (and doesn&#x27;t know where to go to find help). </p><p id="8cc46d0c-8ad2-4f0d-a9d1-2abe6527cf35" class="">This book tries to be different. While you do need some advanced mathematical ideas to understand modern causal inference, there&#x27;s no reason they need to inaccessible. Rigor and clarity don&#x27;t have to be opposing goals. To that end, I do my best to describe these ideas using <strong>natural english prose</strong> (relating to concrete examples) as well as <strong>a </strong><strong><em>lot</em></strong><strong> of pictures and diagrams</strong>. There&#x27;s plenty of mathematical notation in this book, but my hope is that by the time you see a symbol you will understand what it <em>means. </em>I also try to keep the notation as light as possible, omitting complexities that do not serve a didactic purpose and highlighting the main ideas. I try to provide many links and back-references to outside sources so that if you run across something you&#x27;re not familiar with, you know where to start to learn more about it. I know these approaches work because I have used them in the classroom with students and mentees for years.</p><p id="ee42b914-e1ab-4fbe-8ff4-49e08c75182c" class="">I&#x27;ve also tried to keep thing as short and concise as possible in this book by <strong>focusing on core ideas</strong> and a limited number of examples. The point of this is to emphasize what is most important and to allow you to learn the fundamentals without getting distracted. That means there are many topics that aren&#x27;t covered in this book (see section below), but the idea is that this book will give you the tools and mindset you need in order to continue your learning productively. A spelling lesson is of no use until we have mastered the alphabet.</p><p id="72ea03ce-d1af-45ee-be0b-c1810a281c6e" class="">The <strong>voice </strong>I use throughout this book is informal and decidedly nonacademic. I write in the first person and address you, the reader, directly. My figures are hand-drawn and cartoonish. I provide links rather than formal citations. I provide my personal commentary alongside agnostic technical material. All of this is deliberate and meant to help you feel comfortable and <em>guided</em> through this world, which is probably unfamiliar to you to some extent. It’s part of what makes this book different than other existing resources. It’s <strong>a purely </strong><strong><em>educational </em></strong><strong>product</strong>, not a scholarly one.</p><h1 id="5477b83f-2954-4e42-a823-fd5beda84472" class="">What&#x27;s in this book?</h1><p id="c0697344-f582-4bfb-9a3a-5f8f9f2d0188" class="">This book aims to be a relatively self-contained treatment of the modern approach to causal inference. As I said above, the goal is for you to be able to take a vaguely-defined scientific question, translate it into a formal statistical problem, and solve it using a tool that is optimally constructed for the job at hand.</p><p id="5440bced-4c61-46aa-9cec-4acc6036f44f" class="">To do this, we need to first understand the process of <em><strong>statistical inference</strong></em>. We do that by defining a set of assumptions that represent what we know is true about the real world along with some true quantity of interest. Only then do we propose some kind of method or algorithm that, given data, produces an estimate of this quantity. We should be able to show that, with enough data, our answer gets closer and closer to the truth and that we have some way to quantify the uncertainty due to random sampling. These are the topics we&#x27;ll work through in chapter 1, closing out with an example that illustrates the danger of blindly applying popular methods without thinking through the question and assumptions.</p><p id="84cfac56-2c2d-48f6-8d05-81c870b37005" class="">Once we&#x27;ve understood the goal and process of inference, we introduce causality. Causal inference questions are just statistical inference questions but in an imaginary world where we can always observe the result of alternative &quot;what if&quot;s. Of course in the real world we can&#x27;t see all the &quot;what if&quot;s. We only get to see what happened, not what <em>could</em> have happened had we made a different choice, taken a different drug, or implemented a different policy. To link the two worlds together we have to make some assumptions to guarantee that the answer to a statistical inference problem in the real world actually represents the answer to our causal question in the world of &quot;what if&quot;s. Chapter 2 lays out this process (<strong><em>causal identification</em></strong>) and gives examples. Chapter 2 also addresses why a well-defined intervention is important to formulating something as a causal question and discusses broadly applicable methods that quantify the impact of potentially erroneous identifying assumptions.</p><p id="d1e11c41-41eb-4fc9-b2e8-9a882adfb6e2" class="">Once we have an identification result in hand, we are completely done with causality and can focus on solving the real-world inference problem. But for better or for worse there are thousands of inference methods, each of which work under different assumptions and target different parameters. Choosing among these is daunting and often puts the cart before the horse because we&#x27;re forced to define our question such that an existing method can solve it. Chapter 3 lays the foundation to address this problem by showing that for a wide range of assumptions and questions, there is generally a way to quantify what the best possible estimator is in terms of the <strong><em>efficient influence function</em></strong><em>. </em>The efficient influence function completely describes the behavior of the optimal estimator in large samples. This optimal estimator always converges to the right answer with more data and has the lowest legitimate uncertainty possible under the given assumptions. We work through examples of deriving efficient influence functions for the average treatment effect in standard observational and randomized experiments.</p><p id="db9bfed0-005a-44b7-a084-ffd512c670bf" class="">Finally, chapter 4 does the hard work of putting the theory from chapter 3 into practice. Here we show how we can actually build an <em><strong>efficient estimator</strong></em>- one which has the efficient influence function. In other words, given a statistical problem, we can construct an optimal method to solve it. We outline the three main strategies for doing this, which are called bias-correction, estimating equations, and targeted maximum likelihood. We show the different ways in which these strategies ensure that the constructed estimator has the right behavior. All of them ultimately attain the same goal and are theoretically equivalent in large-enough samples, but the differences are relevant for how the constructed estimators perform in practice.</p><h1 id="794d3642-6573-41be-9b45-2053a0822643" class="">Who is it for?</h1><p id="838680bb-e0e7-4586-9a01-579fa6393dec" class="">This book is for anyone who is interested in using data to infer causality in a structured, rigorous way. You don&#x27;t have to be working in or studying any particular field for this material to be useful to you. Clinicians, economists, psychologists, data scientists, biologists... all are welcome. </p><p id="8c5a1540-1429-4409-ab53-fce325914f55" class="">One of the distinguishing features of this book is that it has very few prerequisites. Despite this, you will need some background material to get the most out of this book. The &quot;hard&quot; prerequisites listed are non-negotiable (this book will make no sense to you at all if you don&#x27;t know what a probability distribution is), but the &quot;soft&quot; prerequisites are completely optional. I don&#x27;t cover these topics here because there are already so many good resources to learn them. No need to reinvent the wheel. I provide the curated references below so you know where to go to fill in the holes in your understanding as you go along.</p><h3 id="bf04ef60-09fd-44f8-b7b5-2402a8c047fb" class="">Hard Prerequisites </h3><p id="dd398525-6600-4b2c-9d97-0574c9696f26" class="">You&#x27;ll need to know probability theory at the undergraduate level. Random variables, expectation, conditional expectation, probability densities should be familiar to you, and ideally you have at least a vague understanding of what convergence in distribution means. </p><p id="fadb646a-01bd-4dc3-9503-8d0c14be460a" class="">You also need undergraduate multivariable calculus and linear algebra: integrals, derivatives, vectors, etc. should all be well-worn tools at your disposal. </p><p id="23a077dd-1da1-4695-b7ca-b2595184485f" class="">If you&#x27;re shaky on these topics, I recommend <a href="https://www.khanacademy.org/">Khan Academy</a>. I also encourage you to check out the real analysis and measure-theoretic probability resources that I list below, which will provide you with a much more useful, conceptual understanding.</p><h3 id="c48190eb-8082-4c12-bf3c-c363fbcdb3c9" class="">Soft Prerequisites</h3><p id="b59e80ca-4699-407a-b064-aa1446c22626" class="">The following topics aren&#x27;t necessary to grasp the arc of the arguments I&#x27;ll make, but a fully rigorous understanding isn&#x27;t possible without them. As we go along, I&#x27;ll provide parenthetical commentary to elucidate concepts from these subjects that you may not be familiar with. I suggest attempting to read and understand this book all the way through and taking notes on the places where you feel like you&#x27;re missing something. This will help you prioritize your time if you choose to fill in the gaps using some of the foundational resources I mention below. None of this material is beyond you, I promise. It just takes a little time to work through it. </p><p id="d12133d5-dccb-4a34-84fc-4b69057a51a1" class="">These books and videos give what I think is the most accessible, self-contained treatment of their respective topics, so I recommend sticking with them among the available alternatives. The reason I know this is that I learned everything I know about this stuff by reading them. I tried many books for each topic and these are the ones that I found easiest to work with on my own. They tend to have a more conversational tone, provide more background and intuition to the reader, and have plenty of worked examples. That said, if you find something else that you like, please let me know about it. These subjects should be tackled in the order they are listed here:</p><table id="5750dd36-7435-4fd6-99fd-8577fc06a6f1" class="simple-table"><thead class="simple-table-header"><tr id="e260011f-5a0d-43c9-b78c-e43b88ef9369"><th id="?&lt;Iu" class="simple-table-header-color simple-table-header" style="width:276px">Field</th><th id="VdDo" class="simple-table-header-color simple-table-header" style="width:346px">Topics to Focus On</th><th id="u}iE" class="simple-table-header-color simple-table-header" style="width:200px">Resources</th></tr></thead><tbody><tr id="abac4b4e-4dba-4aa9-b0d4-19fbc5dcebe5"><td id="?&lt;Iu" class="" style="width:276px">1. Intro Real Analysis</td><td id="VdDo" class="" style="width:346px"><mark class="highlight-gray">sequences, limits, series, continuity, convergence (pointwise, uniform), derivative</mark></td><td id="u}iE" class="" style="width:200px"><em><a href="https://www.math.ucdavis.edu/~babson/MAT127B/abbott-second-edition.pdf">Understanding Analysis</a></em><a href="https://www.math.ucdavis.edu/~babson/MAT127B/abbott-second-edition.pdf"> (Ch 1-6)</a> - Abbott</td></tr><tr id="d54b288e-0b6d-4d67-bd5f-8751073430a5"><td id="?&lt;Iu" class="" style="width:276px">2. Measure-Theoretic Probability</td><td id="VdDo" class="" style="width:346px"><mark class="highlight-gray">definition and construction of measure, Lebesgue integral and its properties, Radon-Nikodym theorem, change-of-variables formula, modes of convergence for random variables, independence, central limit theorem, conditional expectation</mark></td><td id="u}iE" class="" style="width:200px"><a href="https://www.youtube.com/playlist?list=PLo4jXE-LdDTQq8ZyA8F8reSQHej3F6RFX"><em>Measure Theory Youtube Lectures</em></a> - Landim
<a href="https://s2pnd-matematika.fkip.unpatti.ac.id/wp-content/uploads/2019/03/Marek-Capinski-Peter-E.-Kopp-Measure-integral-and-probability-Springer-2004.pdf"><em>Measure, Integral, and Probability</em></a> - Capiński &amp; Kopp</td></tr><tr id="e17e3572-0beb-45c2-9c41-7d31deadbeef"><td id="?&lt;Iu" class="" style="width:276px">3. Functional Analysis in <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.2/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathcal L_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span></td><td id="VdDo" class="" style="width:346px"><mark class="highlight-gray">projection, bounded dual space, Reisz representation</mark></td><td id="u}iE" class="" style="width:200px"><a href="https://physics.bme.hu/sites/physics.bme.hu/files/users/BMETE15AF53_kov/Kreyszig%20-%20Introductory%20Functional%20Analysis%20with%20Applications%20(1).pdf"><em>Introductory Functional Analysis with Applications</em></a><a href="https://physics.bme.hu/sites/physics.bme.hu/files/users/BMETE15AF53_kov/Kreyszig%20-%20Introductory%20Functional%20Analysis%20with%20Applications%20(1).pdf"> (Ch 1-3)</a> - Kreyszig</td></tr></tbody></table><p id="8898e754-93e7-484e-ad03-e671870ea272" class="">A few tips for self study: I recommend reading skimming each chapter first and getting an idea of what the main ideas are and how they interconnect to each other and to what you&#x27;ve already done. Take some time to understand what the <em>objects</em> are that are used in each theorem (e.g. what exactly is a sequence / random variable / linear bounded functional?). Think of some examples and draw pictures. Then go through the chapter again and read it slowly, line by line. Reading a line of math should take you as long as it takes to read a paragraph of english because that&#x27;s often how much information is conveyed therein. Now draw some pictures or think of examples attempting the illustrate the content of the theorems. After you&#x27;re done, do several of the exercises that the author provides. I&#x27;ve found that it also helps to make a &quot;one-page summary&quot; or &quot;cheatsheet&quot; of all the material in the chapter (definitions, important theorems, and their relationships). This will help you keep all the concepts organized as you go along.</p><p id="d6bdc43e-d8a6-414c-891d-b2be2a9d6c48" class="">I&#x27;ll also refer to <a href="https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D"><em>Asymptotic Statistics</em></a> by A.W. van der Vaart in certain places throughout the guide where I need to point to technical material. When I do this, I&#x27;ll cite it as &quot;vdV 1998&quot;. I think this book goes a bit too fast in most places but it does contain all the necessary information and details. For better or worse I don&#x27;t think a more readable alternative exists, but let me know if you find one. If you&#x27;ve read the material in the table above and you&#x27;re feeling up to it you can try your hand at ch. 1, 2, 6, 7, 8, and 25 of this book.</p><h1 id="5c0297e8-c61f-4149-b71f-21ad21c4411f" class="">What&#x27;s <em>not</em> in this book?</h1><p id="3dd9af14-802d-40a7-a769-099a99a0f199" class="">I&#x27;ve done my best to keep this book lean and targeted on foundational concepts, which I strongly believe benefits the learner. This book is a starting point for newcomers, not an encyclopedic reference for sages. As a consequence, there are many important and interesting topics that I will say nothing or little about. For the sake of completeness, I&#x27;ll list some of them here so you know where to start looking when you&#x27;re ready to build on your solid foundation. </p><p id="005dcef2-5eb4-4761-8915-838db6a2c036" class="">To keep things simple and consistent, most of the examples in this book focus on inference of the average treatment effect in simple covariate-treatment-outcome observational or randomized studies. However, there are many data structures which are much more elaborate than this. For example, I do not fully discuss inference in longitudinal settings with time-varying covariates and treatments, inference when some outcomes are censored, stochastic interventions, or continuous treatments. Non-IID data (e.g. treatment spillover in a network) is another interesting topic that is not covered. Nonetheless, what is in this book will prepare you to study all of these topics and to understand them as special cases or extensions of the general approach presented here. I believe this is of greater benefit to the learner than presenting an uncontextualized zoo of topics.</p><p id="77798b8e-bfa5-4316-a154-ef4ea24eeac0" class="">While the tools I describe in this book are extremely powerful, they are not <em>all</em>-powerful. Some estimands of legitimate interest are not &quot;pathwise differentiable&quot; and therefore cannot be analyzed with these tools. An important example of such an estimand is the conditional average treatment effect, which is the subject of extensive research. There is not yet a unified approach to these problems.</p><p id="afbe25be-f941-40bb-a56f-7bcd1fbe3cb1" class="">Another interesting and useful setting that we do not discuss is online (or reinforcement) learning. Many topics related to this literature (e.g. off-policy evaluation) are directly addressable with the methods in this book, but here we omit all discussion of scenarios where the treatment policy can be manipulated during data collection as more information comes in. The framework for causal inference we develop in this book does extend to such settings but only with some added complexity that is best understood after learning the basics. </p><p id="9463d598-8390-446b-8bac-b62bcac09034" class="">Lastly, the reader should be aware that this book discusses the most popular and widely used framework for causal inference, which is that of potential outcomes in the frequentist population model. Readers may have heard of directed acyclic graphs (DAGs) as an alternative to potential outcomes, but these two frameworks are in fact equivalent and <a href="https://csss.uw.edu/files/working-papers/2013/wp128.pdf">easily unified</a>. Alternatives are exceedingly rare. Potential outcomes are also used outside the population model in the randomization inference framework, which differs in that only the treatment assignment is assumed to be random (i.e. sampling from a larger population is a source of variability). Much of what you learn here is also applicable to randomization inference. Potential outcomes are also commonly used in Bayesian approaches to causal inference, which we don’t discuss here.</p><h1 id="f08da3af-a4a5-428e-b48e-6b7de4335373" class="">Credit Due</h1><p id="5cf3820b-9cb8-4ba8-a6c5-282292b28efc" class=""><strong>No part of what I present in this book is an original idea of my own.</strong> My contribution here is to only to organize these ideas in a way that they are digestible, contextualized and accessible. As we go through the topics in this book I’ll do my best to point you to the primary sources so the originators of these ideas can have their due credit. I try to keep these references compact in the cover page of each chapter so that you can focus on understanding the material without distraction. However, do be aware that in many cases the ideas were built up layer-by-layer over the years by multiple people and it can sometimes be difficult to give a completely precise attribution. In these cases the best I can do is be clear about where <em>I </em>first encountered the idea in its most fleshed-out form. If you find any places where I could do a better job, please get in touch and let me know!</p><h1 id="530c6206-2f7f-428e-864b-f6b4d49dc0e0" class="">Alternatives</h1><p id="4bb9180e-ba98-4f7c-b567-9446c746a37b" class="">If you find this book isn’t for you, that’s great! Here is a list of other resources that share a similar perspective and cover some of the same ground:</p><ul id="4c1df733-a74a-4c3c-ae3a-065ee4235b44" class="bulleted-list"><li style="list-style-type:disc"><a href="https://arxiv.org/pdf/2203.06469.pdf"><em>Semiparametric Doubly Robust Targeted Double Machine Learning: A Review </em></a><a href="https://arxiv.org/pdf/2203.06469.pdf">- Edward Kennedy</a> </li></ul><ul id="fef689eb-a104-49b0-bc83-e23dcd842643" class="bulleted-list"><li style="list-style-type:disc"><a href="https://link.springer.com/book/10.1007/0-387-37345-4"><em>Semiparametric Theory and Missing Data</em></a><a href="https://link.springer.com/book/10.1007/0-387-37345-4"> - Anastasios Tsiatis</a></li></ul><ul id="671c761e-8143-44c7-80ab-8cf95850cb51" class="bulleted-list"><li style="list-style-type:disc"><a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"><em>Targeted Learning</em></a><a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"> - Mark van der Laan, Sherri Rose</a></li></ul><ul id="5e3bd2ee-afae-47d9-b8a5-057ca1d67c2b" class="bulleted-list"><li style="list-style-type:disc"><a href="https://link.springer.com/book/10.1007/978-0-387-21700-0"><em>Unified Methods for Censored Longitudinal Data and Causality</em></a><a href="https://link.springer.com/book/10.1007/978-0-387-21700-0"> - Mark van der Laan, James Robins</a></li></ul><ul id="ccaf8987-46a0-4ac7-81f7-7b4442c8c7d8" class="bulleted-list"><li style="list-style-type:disc"><a href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2021/03/ciwhatif_hernanrobins_30mar21.pdf"><em>Causal Inference: What If?</em></a><a href="https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2021/03/ciwhatif_hernanrobins_30mar21.pdf"> - Miguel Hernán, James Robins</a></li></ul><p id="3415689d-c7f1-4bd7-8116-8dc4c92888b6" class="">
</p><hr id="94926dfe-3b9f-4139-9ea7-2f52e7081b0d"/><figure class="block-color-blue_background callout" style="white-space:pre-wrap;display:flex" id="eb0b5fd3-3dab-49c7-9a2e-04be60d4ab06"><div style="font-size:1.5em"><span class="icon">➡️</span></div><div style="width:100%"><strong><a href="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/1%20Inference%20and%20Statistics%20b5048808d4154272afe1a93142fd8e9d.html">Chapter 1: Inference and Statistics</a></strong></div></figure><ul id="57337071-a916-47be-a475-cdbc8c38c5f2" class="block-color-gray toggle"><li><details open=""><summary><em>Chapters in this book</em></summary><figure id="b5048808-d415-4272-afe1-a93142fd8e9d" class="link-to-page"><a href="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/1%20Inference%20and%20Statistics%20b5048808d4154272afe1a93142fd8e9d.html">1. Inference and Statistics</a></figure><figure id="a4daf0a8-dd35-4ffe-a151-c0964b1612e2" class="link-to-page"><a href="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/2%20Causality%20and%20Identification%20a4daf0a8dd354ffea151c0964b1612e2.html">2. Causality and Identification</a></figure><figure id="1ebd5e4a-77f0-469e-be6e-df7137988d85" class="link-to-page"><a href="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/3%20Efficiency%20Theory%201ebd5e4a77f0469ebe6edf7137988d85.html">3. Efficiency Theory</a></figure><figure id="488bf545-422a-4d19-a66f-db8826495317" class="link-to-page"><a href="Modern%20Causal%20Inference%2032b4128f336d4e4794fce56d2de18ec5/4%20Building%20Efficient%20Estimators%20488bf545422a4d19a66fdb8826495317.html">4. Building Efficient Estimators</a></figure><p id="64c008cb-2755-4f50-95b4-ee1924f207da" class="">
</p></details></li></ul><p id="ddb2e668-9cd9-409e-b0d9-85c92011536a" class="">
</p></div></article></body></html>